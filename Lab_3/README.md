### Задание 3 - 10 баллов

- Загрузить набор данных [Spam Or Not Spam](https://www.kaggle.com/datasets/ozlerhakan/spam-or-not-spam-dataset) (или любой другой, какой вам нравится)
- Обучить модели и сравнить различные способы векторизации с помощью внутренней оценки (intrinsic):
  - Word2Vec SkipGram / CBOW (параметр sg в `gensim.models.word2vec.Word2Vec`) - **3 балла**
  - fastText (можно взять в gensim, или в fasttext как на семинаре) - **2 балла**
- Обучить на полученных векторах модели LogisticRegression и сравнить качество на отложенной выборке - **2 балла**

- Обеспечена воспроизводимость решения: зафиксированы random_state, ноутбук воспроизводится от начала до конца без ошибок - **2 балла**

- Соблюден code style на уровне pep8 и [On writing clean Jupyter notebooks](https://ploomber.io/blog/clean-nbs/)  - **1 балл**
 
Примечания:

- Для получения более качественных эмбеддингов стоит предварительно сделать предобработку корпуса - отсеять стоп-слова, провести нормализацию и тп. Предобработка рассматривалась в первой лекции/семинаре
- В данном случае под intrinsic оценкой подразумевается просто использование методов `most_similar`, `doesnt_match`. Однако, если есть желание, можно измерить косинусное расстояние между отдельными парами слов и проверить, есть ли корреляция с корпусами для intrinsic-оценки, которые обсуждались на семинаре